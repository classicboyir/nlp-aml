{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657031460532
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import azureml.core\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from IPython.core.display import HTML\n",
    "from utils import *\n",
    "\n",
    "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.integrations import AzureMLCallback\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "# from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.path.join(os.getcwd(), \"..\"), 'project'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_transformer import get_model, adjust_tokenizer, compute_metrics, get_encode_labels, tokenize_function, generate_tokenized_dataset, get_datasets, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657031937527
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657031511243
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ds_X_train = Dataset.get_by_name(ws, name=\"owner_g_classfication_train\", version=8)\n",
    "ds_X_val = Dataset.get_by_name(ws, name=\"owner_g_classfication_val\", version=8)\n",
    "ds_X_test = Dataset.get_by_name(ws, name=\"owner_g_classfication_test\", version=8)\n",
    "ds_temporal_test = Dataset.get_by_name(ws, name=\"owner_g_classfication_temporal_test\", version=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657031511266
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f'{ds_X_train.tags}: V{ds_X_train.version}')\n",
    "print(f'{ds_X_val.tags}: V{ds_X_val.version}')\n",
    "print(f'{ds_X_test.tags}: V{ds_X_test.version}')\n",
    "print(f'{ds_temporal_test.tags}: V{ds_temporal_test.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657031511284
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pdf_X_train = ds_X_train.to_pandas_dataframe()\n",
    "pdf_X_val = ds_X_val.to_pandas_dataframe()\n",
    "pdf_X_test = ds_X_test.to_pandas_dataframe()\n",
    "pdf_temporal_test = ds_temporal_test.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'pdf_X_train shape: {pdf_X_train.shape}')\n",
    "print(f'pdf_X_val shape: {pdf_X_val.shape}')\n",
    "print(f'pdf_X_test shape: {pdf_X_test.shape}')\n",
    "print(f'pdf_temporal_test shape: {pdf_temporal_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_checkpoint = \"bert-base-uncased\"\n",
    "text_field_name = \"txt_field\"\n",
    "target_name = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = 'model_output/model'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_directory, num_labels=51)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=joblib.load(model_directory + '/labelEncoder.joblib')\n",
    "le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [text_field_name, target_name, 'labels']\n",
    "\n",
    "train_ds, tokenized_train_ds = generate_tokenized_dataset(pdf_X_train, fields, le, target_name, text_field_name, tokenizer)\n",
    "validation_ds, tokenized_validation_ds = generate_tokenized_dataset(pdf_X_val, fields, le, target_name, text_field_name, tokenizer)\n",
    "test_ds, tokenized_test_ds = generate_tokenized_dataset(pdf_X_test, fields, le, target_name, text_field_name, tokenizer)\n",
    "temporal_test_ds, tokenized_temporal_test_ds = generate_tokenized_dataset(pdf_temporal_test, fields, le, target_name, text_field_name, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('custom_model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, compute_metrics=compute_metrics, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainer.predict(tokenized_test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "\n",
    "run = Run.get_context(allow_offline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = test_model(trainer, tokenized_test_ds, 'test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_result = test_model(trainer, tokenized_temporal_test_ds, 'temporal_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pred = best_model.predict(pdf_X_test['txt_field'].to_frame())\n",
    "pdf_X_test['pred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_X_test = pd.read_csv('pdf_X_test_v8.csv')\n",
    "pdf_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_X_test, df_result = get_base_dataframes(pdf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_sanky_chart(pdf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_merged_test = calculate_top_OGs(pdf_X_test, df_result)\n",
    "pd_merged_test[['target', 'total_records',\n",
    "       'matched', 'percentage_matched', 'top_2_groups', 'top_2_count',\n",
    "       'top_2_perc']].sort_values('percentage_matched', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_X_val = pd.read_csv('pdf_X_val_v8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_X_val, df_result = get_base_dataframes(pdf_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_sanky_chart(pdf_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_merged_val = calculate_top_OGs(pdf_X_val, df_result)\n",
    "pd_merged_val[['target', 'total_records',\n",
    "       'matched', 'percentage_matched', 'top_2_groups', 'top_2_count',\n",
    "       'top_2_perc']].sort_values('percentage_matched', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_merged_val[['target', 'total_records',\n",
    "       'matched', 'percentage_matched', 'top_2_groups', 'top_2_count',\n",
    "       'top_2_perc']].sort_values('percentage_matched', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_X_train = pd.read_csv('pdf_X_train_v8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_X_train, df_result_train = get_base_dataframes(pdf_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_merged_train = calculate_top_OGs(pdf_X_train, df_result_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_sanky_chart(pdf_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_merged_test[['target', 'total_records',\n",
    "       'matched', 'percentage_matched', 'top_2_groups', 'top_2_count',\n",
    "       'top_2_perc']].sort_values('percentage_matched', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_merged_val.shape, pd_merged_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_eval_test = pd.merge(pd_merged_val, pd_merged_test, suffixes=(\"_evaluation\", \"_test\"), on='target', how='left')\n",
    "pd_eval_test['diff'] = abs(pd_eval_test['percentage_matched_test'] - pd_eval_test['percentage_matched_evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_eval_test[['target', 'diff', 'percentage_matched_test', 'percentage_matched_evaluation', 'top_2_groups_evaluation', 'top_2_count_evaluation', 'top_2_perc_evaluation',\n",
    "             'top_2_groups_test', 'top_2_count_test', 'top_2_perc_test']].sort_values('diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_eval_test = pd.merge(pd_merged_train, pd_eval_test, on='target', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_eval_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_eval_test['diff_val'] = abs(pd_train_eval_test['percentage_matched'] - pd_train_eval_test['percentage_matched_evaluation'])\n",
    "pd_train_eval_test['diff_test'] = abs(pd_train_eval_test['percentage_matched'] - pd_train_eval_test['percentage_matched_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_train_eval_test[['target', 'diff_val', 'diff_test',\n",
    "                    'percentage_matched', 'percentage_matched_test', 'percentage_matched_evaluation', \n",
    "                    'top_2_groups', 'top_2_count', 'top_2_perc',\n",
    "                    'top_2_groups_evaluation', 'top_2_count_evaluation', 'top_2_perc_evaluation',\n",
    "                    'top_2_groups_test', 'top_2_count_test', 'top_2_perc_test']].sort_values('percentage_matched', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_temporal_test = pd.read_csv('pdf_temporal_test_v8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_temporal_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdf_temporal_test, df_result_temporal = get_base_dataframes(pdf_temporal_test)\n",
    "pd_merged_temporal = calculate_top_OGs(pdf_temporal_test, df_result_temporal)\n",
    "draw_sanky_chart(pdf_temporal_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_merged_temporal[['target', 'total_records',\n",
    "       'matched', 'percentage_matched', 'top_2_groups', 'top_2_count',\n",
    "       'top_2_perc']].sort_values('percentage_matched', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_performance(pdf_X_train['target'], pdf_X_train['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_performance(pdf_X_val['target'], pdf_X_val['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_performance(pdf_X_test['target'], pdf_X_test['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_performance(pdf_temporal_test['target'], pdf_temporal_test['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(pdf_temporal_test['target'], pdf_temporal_test['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_temporal_eval_test = pd.merge(pd_merged_temporal, pd_eval_test, on='target', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_temporal_eval_test['diff_val'] = abs(pd_temporal_eval_test['percentage_matched'] - pd_temporal_eval_test['percentage_matched_evaluation'])\n",
    "pd_temporal_eval_test['diff_test'] = abs(pd_temporal_eval_test['percentage_matched'] - pd_temporal_eval_test['percentage_matched_test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_temporal_eval_test[['target', 'diff_val', 'diff_test',\n",
    "                    'percentage_matched', 'percentage_matched_test', 'percentage_matched_evaluation', \n",
    "                    'top_2_groups', 'top_2_count', 'top_2_perc',\n",
    "                    'top_2_groups_evaluation', 'top_2_count_evaluation', 'top_2_perc_evaluation',\n",
    "                    'top_2_groups_test', 'top_2_count_test', 'top_2_perc_test']].sort_values('percentage_matched', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.integrations import AzureMLCallback\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "sys.path.append(os.path.join(os.path.join(os.getcwd(), \"..\"), 'project'))\n",
    "from train_transformer import get_model, adjust_tokenizer, compute_metrics, get_encode_labels, tokenize_function, generate_tokenized_dataset, get_datasets, test_model\n",
    "from utils import *\n",
    "# from utils import get_valid_runs, get_highest_performing_model, get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml\n",
    "import mlflow\n",
    "from azureml.core import Workspace, Dataset, Environment\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "print(\"MLflow version:\", mlflow.version.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "# mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "script_folder = './project'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='transformer_hp')\n",
    "# mlflow.set_experiment('transformer_hp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take up to 10 minutes\n",
    "counter = 0\n",
    "best_temporal_f1_weighted = 0.0\n",
    "\n",
    "metric_name = \"temporal_test_f1_weighted\"\n",
    "dic_runs = get_valid_runs(exp, metric_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_groups = '120'\n",
    "\n",
    "best_performing_run = get_highest_performing_model(dic_runs, metric_name, top_groups)\n",
    "\n",
    "dic_datasets = get_dataset(best_performing_run)\n",
    "pdf_train = dic_datasets['ds_train'].to_pandas_dataframe()\n",
    "pdf_temporal_test = dic_datasets['ds_temporal_test'].to_pandas_dataframe()\n",
    "\n",
    "pdf_temporal_test = predict(best_performing_run[\"run\"], pdf_train, pdf_temporal_test, top_groups, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_temporal_test_preped, df_temp_test_result = get_base_dataframes(pdf_temporal_test)\n",
    "draw_sanky_chart(pdf_temporal_test_preped, top_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_acc_per_class = calculate_top_groups(pdf_temporal_test_preped, df_temp_test_result, pdf_train)\n",
    "pdf_acc_per_class_final = pdf_acc_per_class[['target', 'total_records',\n",
    "       'matched', 'percentage_matched', 'top_2_group', 'top_2_count',\n",
    "       'top_2_perc', 'count_in_training_set', 'count_ratio', 'text_length_average']].sort_values('percentage_matched', ascending=False)\n",
    "pdf_acc_per_class_final.to_csv(f'output/pdf_acc_per_class_final{top_groups}.csv', index=False)\n",
    "plot_accuracy_count_plot(pdf_acc_per_class_final, dic_datasets['ds_train'], top_groups).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_groups = '100'\n",
    "\n",
    "best_performing_run = get_highest_performing_model(dic_runs, metric_name, top_groups)\n",
    "\n",
    "dic_datasets = get_dataset(best_performing_run)\n",
    "pdf_train = dic_datasets['ds_train'].to_pandas_dataframe()\n",
    "pdf_temporal_test = dic_datasets['ds_temporal_test'].to_pandas_dataframe()\n",
    "\n",
    "pdf_temporal_test = predict(best_performing_run[\"run\"], pdf_train, pdf_temporal_test, top_groups, 'target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_temporal_test_preped, df_temp_test_result = get_base_dataframes(pdf_temporal_test)\n",
    "draw_sanky_chart(pdf_temporal_test_preped, top_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_acc_per_class = calculate_top_groups(pdf_temporal_test_preped, df_temp_test_result, pdf_train)\n",
    "pdf_acc_per_class_final = pdf_acc_per_class[['target', 'total_records',\n",
    "       'matched', 'percentage_matched', 'top_2_group', 'top_2_count',\n",
    "       'top_2_perc', 'count_in_training_set', 'count_ratio', 'text_length_average']].sort_values('percentage_matched', ascending=False)\n",
    "pdf_acc_per_class_final.to_csv(f'output/pdf_acc_per_class_final{top_groups}.csv', index=False)\n",
    "plot_accuracy_count_plot(pdf_acc_per_class_final, dic_datasets['ds_train'], top_groups).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
